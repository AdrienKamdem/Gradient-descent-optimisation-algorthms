# Gradient-descent-optimisation-algorthms

I build different functions using machine learning optimizations. Optimization is one of the most important mathematical subject in ML. We absolutely need it for hyperparameter optimization to tune the model. By finding the optimal combination of their values, we can decrease the error and build the most accurate model. 

So I build 3 differents ML well-known optimizations functions.

- The first is a function using the optimal step for a quadratic funtion algorithm
- The second one is a function using the optimal step for a least squares function algorithm (which also is a quadratic function 'particular case')
- The third and last one is a function using the Armijo optimal step algorithm (completely different from the two function above by its stop condition)

All these functions are only returning the 'optimal step' which was the goal of my school project but by building this 'optimal step' we also have at the last iteration 
the minimum of the function (which is the final result a machine learning engineer want) and the descent direction which are both as important.
